# Awesome Vision Foundation Models Unifying Understanding and Generation
Solving all language tasks and exploring scaling laws with LLMs are so cool. *How about building vision foundation models?* Also, we are now good at handling vision tasks seperately. *How about unifying vision understanding and generation?*  
As the long-term roadmap for building a generalist foundation model to solve all vision tasks has not yet been fully determined, we conduct the first survey to provide comprehensive summary and in-depth analysis on vision foundation models unifying both understanding and generation. This repository provides a curated list of related papers and resources, come take a look and let's share insights on unresolved challenges together! Please stay tuned and give us a üåü if you are interested in our project, we will update the latest advancements both on our paper and this repo continuously.


---

## üî• News
`[2024-12-02]` We build this repo, and a full version of our survey and repo is coming soon.  
`[2024-10-29]` We have released the survey v1: [Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective](https://arxiv.org/abs/2410.22217).

---

## üçª Contributions

Both suggestions about our project and sharing of your work are welcomed! Please feel free to open an issue, pull a request or send us an email.

---


## üìÉ Table of Contents
- [Vision-Foundation-Models-Unifying-Understanding-and-Generation](#-vision-foundation-models-unifying-understanding-and-generation)
  - [Autoregression](#-autoregression)
  - [Diffusion](#-diffusion)
  - [Autoregression and Diffusion](#-autoregression-and-diffusion)
 
---


## Vision Foundation Models Unifying Understanding and Generation

### Autoregression
|  Title  |  Abbreviation  |  Venue  |   Date   |  Organization  |   Open Source  |
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|
| [**Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling**](https://arxiv.org/pdf/2412.05271) | InternVL 2.5 | arXiv | 2024-12-06 | Shanghai AI Lab, SenseTime, THU, NJU, FDU, CUHK, SJTU | [Github](https://github.com/OpenGVLab/InternVL) | - |
| [**JetFormer: An Autoregressive Generative Model of Raw Images and Text**](https://arxiv.org/pdf/2411.19722) | JetFormer | arXiv | 2024-11-29 | Google Deepmind | - | - |
| [**MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding**](https://arxiv.org/pdf/2411.17762) | MUSE-VL | arXiv | 2024-11-26 | ByteDance | - | - |
| [**UniPose: A Unified Multimodal Framework for Human Pose Comprehension, Generation and Editing**](https://arxiv.org/pdf/2411.16781) | UniPose | arXiv | 2024-11-25 | CAS | - | - |
| [**SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE**](https://arxiv.org/pdf/2411.16856) | SAR3D | arXiv | 2024-11-25 | NTU, Shanghai AI Lab | [Github](https://github.com/cyw-3d/SAR3D) | - |
| [**LLaMA-Mesh: Unifying 3D Mesh Generation with Language Models**](https://arxiv.org/pdf/2410.13861) | LLaMA-Mesh | arXiv | 2024-11-14 | THU, NVIDIA | [Github](https://github.com/nv-tlabs/LLaMa-Mesh) | - |
| [**Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation**](https://arxiv.org/pdf/2410.13848) | Janus | arXiv | 2024-10-17 | DeepSeek, HKU, PKU | [Github](https://github.com/deepseek-ai/Janus) | - |
| [**Emu3: Next-Token Prediction is All You Need**](https://arxiv.org/pdf/2409.18869) | Emu3 | arXiv | 2024-09-27 | BAAI | [Github](https://github.com/baaivision/Emu3) | - |
| [**UniMuMo: Unified Text, Music and Motion Generation**](https://arxiv.org/pdf/2409.12191) | UniMuMo | arXiv | 2024-10-06 | CUHK, UW, UBC, UMass Amherst, MIT-IBM Watson AI Lab, Cisco | [Github](https://github.com/hanyangclarence/UniMuMo) | - |
| [**Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution**](https://arxiv.org/pdf/2409.12191) | Qwen2-VL | arXiv | 2024-09-18 | Alibaba | [Github](https://github.com/QwenLM/Qwen2-VL) | - |
| [**VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation**](https://arxiv.org/pdf/2409.04429) | VILA-U | arXiv | 2024-09-06 | THU, MIT, NVIDIA, UC Berkeley, UC San Diego | [Github](https://github.com/mit-han-lab/vila-u) | - |
| [**xGen-MM (BLIP-3): A Family of Open Large Multimodal Models**](https://arxiv.org/pdf/2408.08872) | xGen-MM | arXiv | 2024-08-16 | Salesforce, UW | [Github](https://github.com/salesforce/LAVIS/tree/xgen-mm) | - |
| [**ANOLE: An Open, Autoregressive, Native Large Multimodal Models for Interleaved Image-Text Generation**](https://arxiv.org/pdf/2407.06135) | ANOLE | arXiv | 2024-06-08 | SJTU, Shanghai AI Lab, FDU, GAIR | [Github](https://github.com/GAIR-NLP/anole) | - |
| [**Chameleon: Mixed-Modal Early-Fusion Foundation Models**](https://arxiv.org/pdf/2405.09818) | Chameleon | arXiv | 2024-05-16 | Meta FAIR | [Github](https://github.com/facebookresearch/chameleon) | - |
| [**GPT-4o System Card**](https://arxiv.org/pdf/2410.21276) | GPT-4o | arXiv | 2024-05-13 | OpenAI | - | - |
| [**WorldGPT: Empowering LLM as Multimodal World Model**](https://arxiv.org/pdf/2404.18202) | WorldGPT | MM | 2024-04-28 | ZJU, NUS | [Github](https://github.com/DCDmllm/WorldGPT) | - |
| [**GiT: Towards Generalist Vision Transformer through Universal Language Interface**](https://arxiv.org/pdf/2403.09394) | GiT | ECCV | 2024-03-14 | PKU, Max Planck Institute for Informatics, CUHK, ETH Zurich | [Github](https://github.com/Haiyang-W/GiT) | - |
| [**UniCode: Learning a Unified Codebook for Multimodal Large Language Models**](https://arxiv.org/pdf/2403.09072) | Unicode | ECCV | 2024-03-14 | BAAI, PKU | - | - |
| [**Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization**](https://arxiv.org/pdf/2402.03161) | Video-LaVIT | ICML | 2024-02-05 | PKU, Kuaishou | [Github](https://github.com/jy0205/LaVIT) | - |
| [**Scalable pre-training of large autoregressive image models**](https://openreview.net/pdf?id=c92KDfEZTg) | AIM | ICML | 2024-01-16 | Apple | [Github](https://github.com/apple/ml-aim) | - |
| [**Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision Language Audio and Action**](https://openaccess.thecvf.com/content/CVPR2024/papers/Lu_Unified-IO_2_Scaling_Autoregressive_Multimodal_Models_with_Vision_Language_Audio_CVPR_2024_paper.pdf) | Unified-IO 2 | CVPR | 2023-12-28 | Allen  Institute for AI, UIUC, UW | [Github](https://github.com/allenai/unified-io-2) | - |
| [**Sequential Modeling Enables Scalable Learning for Large Vision Models**](https://openaccess.thecvf.com/content/CVPR2024/papers/Bai_Sequential_Modeling_Enables_Scalable_Learning_for_Large_Vision_Models_CVPR_2024_paper.pdf) | LVM | CVPR | 2023-12-01 | UC Berkeley, JHU | [Github](https://github.com/ytongbai/LVM) | - |
| [**ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model**](https://arxiv.org/pdf/2311.04589) | ShapeGPT | arXiv | 2023-11-29 | FDU, Tencent, ShanghaiTech, Zhejiang Lab | [Github](https://arxiv.org/pdf/2311.17618) | - |
| [**TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models**](https://arxiv.org/pdf/2311.04589) | TEAL | arXiv | 2023-11-08 | Tencent | - | - |
| [**Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization**](https://arxiv.org/pdf/2309.04669) | LaVIT | ICLR | 2023-09-09 | PKU, Kuaishou | [Github](https://github.com/jy0205/LaVIT) | - |
| [**Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks**](https://arxiv.org/pdf/2206.08916) | Unified-IO | ICLR | 2022-06-17 | Allen Institute for AI, UW | [Github](https://github.com/allenai/unified-io-inference) | - |
| [**Write and Paint: Generative Vision-Language Models are Unified Modal Learners**](https://arxiv.org/pdf/2206.07699) | DAVINCI | ICLR | 2022-06-15 | HKUST, ByteDance, SJTU | [Github](https://github.com/shizhediao/DaVinci) | - |
| [**DU-VLG: Unifying Vision-and-Language Generation via Dual Sequence-to-Sequence Pre-training**](https://arxiv.org/pdf/2203.09052) | DU-VLG | ACL | 2022-03-17 | Baidu | - | - |

### Diffusion
|  Title  |  Abbreviation  |  Venue  |   Date   |  Organization  |   Open Source  |
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|
| [**MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks**](https://arxiv.org/pdf/2411.19786) | MoTe | arXiv | 2024-11-29 | HKU, NUS, ZJU | - | - |
| [**LaVin-DiT: Large Vision Diffusion Transformer**](https://arxiv.org/pdf/2411.11505) | LaVin-DiT | arXiv | 2024-11-18 | USYD, NUS, UniMelb, AIsphere | [Github](https://github.com/DerrickWang005/LaVin-DiT) | - |
| [**Unimotion: Unifying 3D Human Motion Synthesis and Understanding**](https://arxiv.org/pdf/2409.15904) | Unimotion | arXiv | 2024-09-24 | Tuebingen U, Max Planck Institute for Informatics | [Github](https://github.com/Coral79/Unimotion) | - |
| [**GenRec: Unifying Video Generation and Recognition with Diffusion Models**](https://arxiv.org/pdf/2408.15241) | GenRec | NeurIPS | 2024-08-27 | FDU, Shanghai Collaborative Innovation Center of Intelligent Visual Computing, UMD | [Github](https://github.com/wengzejia1/GenRec) | - |
| [**One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale**](https://proceedings.mlr.press/v202/bao23a/bao23a.pdf) | UniDiffuser | ICML | 2023-03-12 | THU, Shengshu, RUC, BAAI, Pazhou Lab | [Github](https://github.com/thu-ml/unidiffuser) | - |


### Autoregression and Diffusion
|  Title  |  Abbreviation  |  Venue  |   Date   |  Organization  |   Open Source  |
|:--------|:--------:|:--------:|:--------:|:--------:|:--------:|
| [**ILLUME: Illuminating Your LLMs to See, Draw, and Self-Enhance**](https://arxiv.org/pdf/2412.06673) | ILLUME | arXiv | 2024-12-09 | HUAWEI | - | - |
| [**PUMA: Empowering Unified MLLM with Multi-granular Visual Generation**](https://arxiv.org/pdf/2410.13861) | PUMA | arXiv | 2024-10-17 | CUHK, HKU, SenseTime, Shanghai AI Lab, THU | [Github](https://github.com/rongyaofang/PUMA) | - |
| [**Vitron: A Unified Pixel-level Vision LLM for Understanding, Generating, Segmenting, Editing**](https://openreview.net/pdf?id=kPmSfhCM5s) | Vitron | NeurIPS | 2024-09-26 | Skywork AI, NUS, NTU | [Github](https://github.com/SkyworkAI/Vitron) | - |
| [**MIO: A Foundation Model on Multimodal Tokens**](https://arxiv.org/pdf/2409.17692) | MIO | arXiv | 2024-09-26 | BHU, 01.AI, M-A-P, PolyU, UAlberta, UWaterloo, UoM, CAS, PKU, HKUST | [Github](https://github.com/MIO-Team/MIO) | - |
| [**Show-o: One Single Transformer to Unify Multimodal Understanding and Generation**](https://arxiv.org/pdf/2408.12528) | Show-o | arXiv | 2024-08-22 | NUS, ByteDance | [Github](https://github.com/showlab/Show-o) | - |
| [**Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model**](https://arxiv.org/pdf/2408.11039) | TransFusion | arXiv | 2024-08-20 | Meta, Waymo, USC | [Github](https://github.com/lucidrains/transfusion-pytorch) | - |
| [**Generative Visual Instruction Tuning**](https://arxiv.org/pdf/2406.11262) | GenLLaVA | arXiv | 2024-06-17 | Rice, Google DeepMind | [Github](https://github.com/jeffhernandez1995/GenLlaVA) | - |
| [**VisionLLM v2: An End-to-End Generalist Multimodal Large Language Model for Hundreds of Vision-Language Tasks**](https://arxiv.org/pdf/2406.08394) | VisionLLM v2 | arXiv | 2024-06-12 | Shanghai AI Lab, HKU, THU, BIT, HKUST, NJU, SenseTime | [Github](https://github.com/OpenGVLab/VisionLLM) | - |
| [**UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model**](https://arxiv.org/pdf/2408.02503) | UnifiedMLLM | arXiv | 2024-08-05 | ByteDance, FDU, USTC | [Github](https://github.com/lzw-lzw/UnifiedMLLM) | - |
| [**SEED-X: Multimodal Models with Unified Multi-granularity Comprehension and Generation**](https://arxiv.org/pdf/2404.14396) | SEED-X | arXiv | 2024-04-22 | Tencent | [Github](https://github.com/AILab-CVC/SEED-X) | - |
| [**AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling**](https://arxiv.org/pdf/2402.12226) | AnyGPT | arXiv | 2024-02-19 | FDU, Multimodal Art Projection Research Community, Shanghai AI Lab | [Github](https://junzhan2000.github.io/AnyGPT.github.io/) | - |
| [**Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models**](https://arxiv.org/pdf/2402.03327) |  Uni3D-LLM| arXiv | 2024-01-09 | Shanghai AI Lab, DUT, SDU | - | - |
| [**Generative multimodal models are in-context learners**](https://openaccess.thecvf.com/content/CVPR2024/papers/Sun_Generative_Multimodal_Models_are_In-Context_Learners_CVPR_2024_paper.pdf) | Emu2 | CVPR | 2023-12-20 | BAAI, THU, PKU | [Github](https://github.com/baaivision/Emu/tree/main/Emu2) | - |
| [**VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation**](https://arxiv.org/pdf/2312.09251) | VL-GPT | arXiv | 2023-12-14 | XJTU, Tencent, HKU | [Github](https://github.com/AILab-CVC/VL-GPT) | - |
| [**CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation**](https://openaccess.thecvf.com/content/CVPR2024/papers/Tang_CoDi-2_In-Context_Interleaved_and_Interactive_Any-to-Any_Generation_CVPR_2024_paper.pdf) | CoDi-2 | CVPR | 2023-11-30 | UC Berkeley, Microsoft, Zoom, UNC Chapel Hill | [Github](https://github.com/microsoft/i-Code/tree/main/CoDi-2) | - |
| [**GPT4Point: A Unified Framework for Point-Language Understanding and Generation**](https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_GPT4Point_A_Unified_Framework_for_Point-Language_Understanding_and_Generation_CVPR_2024_paper.pdf) | GPT4Point | CVPR | 2023-12-05 | HKU, CUHK, FDU, SJTU, Shanghai AI Lab | [Github](https://github.com/Pointcept/GPT4Point) | - |
| [**GPT4Video: A Unified Multimodal Large Language Model for lnstruction-Followed Understanding and Safety-Aware Generation**](https://openaccess.thecvf.com/content/CVPR2024/papers/Qi_GPT4Point_A_Unified_Framework_for_Point-Language_Understanding_and_Generation_CVPR_2024_paper.pdf) | GPT4Video | MM | 2023-11-25 | USYD, Tencent | [Github](https://github.com/gpt4video/GPT4Video) | - |
| [**DreamLLM: Synergistic Multimodal Comprehension and Creation**](https://openreview.net/pdf?id=y01KGvd9Bw) | DreamLLM | ICLR | 2023-09-20 | XJTU, IIISCT, MEGVII Technology, THU, HUST, Shanghai AI Lab, Shanghai Qi Zhi Institute | [Github](https://github.com/RunpeiDong/DreamLLM) | - |
| [**NExT-GPT: Any-to-Any Multimodal LLM**](https://arxiv.org/pdf/2309.05519) | NExT-GPT | ICML | 2023-09-11 | NUS | [Github](https://github.com/NExT-GPT/NExT-GPT) | - |
| [**Emu: Generative Pretraining in Multimodality**](https://proceedings.neurips.cc/paper_files/paper/2023/file/43a69d143273bd8215578bde887bb552-Paper-Conference.pdf) | Emu | ICLR | 2023-07-11 | BAAI, THU, PKU | [Github](https://github.com/baaivision/Emu) | - |
| [**Generating Images with Multimodal Language Models**](https://proceedings.neurips.cc/paper_files/paper/2023/file/43a69d143273bd8215578bde887bb552-Paper-Conference.pdf) | GILL | NeurIPS | 2023-05-26 | CMU | [Github](https://github.com/kohjingyu/gill) | - |

---


## ü´∂ Citation
If you find our survey and repo useful, you can cite the paper as following formats. We appreciate it a lot.

### Survey v1: Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective
```BibTeX
@article{xie2024arvfm,
  title={Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective},
  author={Xie, Shenghao and Zu, Wenqiang and Zhao, Mingyang and Su, Duo and Liu, Shilong and Shi, Ruohua and Li, Guoqi and Zhang, Shanghang and Ma, Lei},
  journal={arXiv preprint arXiv:2410.22217},
  year={2024}
}
```

---
