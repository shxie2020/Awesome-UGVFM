# Awesome Vision Foundation Models Unifying Understanding and Generation
LLMs that can solve all language tasks and exhibit the scaling law are very impressive. *How about building vision foundation models?*

Solving vision tasks separately has achieved significant advancements. *How about unifying understanding and generation?*

As the long-term roadmap for building a generalist foundation model to solve all vision tasks has not yet been fully determined, we conduct the first survey to provide comprehensive summary and in-depth analysis on vision foundation models unifying both understanding and generation. This repository provides a curated list of related papers and resources, come take a look and let's share insights on unresolved challenges together! Please stay tuned and give us a ðŸŒŸ if you are interested in our project, we will update the latest advancements continuously.


---

## ðŸ”¥ News
`[2024-10-29]` We have released the survey v1: [Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective](https://arxiv.org/abs/2410.22217).

---


## ðŸ“ƒ Table of Contents
- [Vision-Foundation-Models-Unifying-Understanding-and-Generation](#-vision-foundation-models-unifying-understanding-and-generation)
  - [Autoregression](#-autoregression)
  - [Diffusion](#-diffusion)
  - [Autoregression and Diffusion](#-autoregression-and-diffusion)
 
---


## Vision Foundation Models Unifying Understanding and Generation

### Autoregression
|  Title  |  Venue  |   Date   |  Organization  |   Open Source  |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**JetFormer: An Autoregressive Generative Model of Raw Images and Text**](https://arxiv.org/pdf/2411.19722) | arXiv | 2024-11-29 | Google Deepmind | - | - |
| [**MUSE-VL: Modeling Unified VLM through Semantic Discrete Encoding**](https://arxiv.org/pdf/2411.17762) | arXiv | 2024-11-26 | ByteDance | - | - |
| [**SAR3D: Autoregressive 3D Object Generation and Understanding via Multi-scale 3D VQVAE**](https://arxiv.org/pdf/2411.16856) | arXiv | 2024-11-25 | NTU, Shanghai AI Lab | [Github](https://github.com/cyw-3d/SAR3D) | - |
| [**Chameleon: Mixed-Modal Early-Fusion Foundation Models**](https://arxiv.org/pdf/2405.09818) | arXiv | 2024-05-16 | Meta FAIR | [Github](https://github.com/facebookresearch/chameleon) | - |

### Diffusion
|  Title  |  Venue  |   Date   |  Organization  |   Open Source  |
|:--------|:--------:|:--------:|:--------:|:--------:|
| [**MoTe: Learning Motion-Text Diffusion Model for Multiple Generation Tasks**](https://arxiv.org/pdf/2411.19786) | arXiv | 2024-11-29 | HKU, NUS, ZJU | - | - |
| [**LaVin-DiT: Large Vision Diffusion Transformer**](https://arxiv.org/pdf/2411.11505) | arXiv | 2024-11-18 | USYD, NUS, UniMelb, AIsphere | [Github](https://github.com/DerrickWang005/LaVin-DiT) | - |
| [**Unimotion: Unifying 3D Human Motion Synthesis and Understanding**](https://arxiv.org/pdf/2409.15904) | arXiv | 2024-09-24 | Tuebingen U, MPI INF | [Github](https://github.com/Coral79/Unimotion) | - |
| [**One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale**](https://proceedings.mlr.press/v202/bao23a/bao23a.pdf) | ICML | 2023-03-12 | THU, Shengshu, RUC, BAAI, Pazhou Lab | [Github](https://github.com/thu-ml/unidiffuser) | - |


### Autoregression and Diffusion
|  Title  |  Venue  |   Date   |  Organization  |   Open Source  |
|:--------|:--------:|:--------:|:--------:|:--------:|

---


## ðŸ«¶ Citation
If you find our survey and repo useful, you can cite the paper as following formats. We appreciate it a lot.

### Survey v1: Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective
```BibTeX
@article{xie2024arvfm,
  title={Towards Unifying Understanding and Generation in the Era of Vision Foundation Models: A Survey from the Autoregression Perspective},
  author={Xie, Shenghao and Zu, Wenqiang and Zhao, Mingyang and Su, Duo and Liu, Shilong and Shi, Ruohua and Li, Guoqi and Zhang, Shanghang and Ma, Lei},
  journal={arXiv preprint arXiv:2410.22217},
  year={2024}
}
```

---
